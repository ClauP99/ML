{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los himnos nacionales son piezas musicales que representan la identidad y el orgullo de una nación, con letras que reflejan su historia, cultura y valores. Estos himnos varían considerablemente en estilo y contenido, desde melodías solemnes hasta composiciones enérgicas y vibrantes.\n",
    "\n",
    "Algunos himnos tienen letras más militaristas, como el de Francia, que evocan un sentido de lucha y defensa. Otros himnos son más patrióticos y destacan la rica naturaleza y belleza del país, como el de Brasil. \n",
    "\n",
    "Esto lleva a pensar que las letras de los himnos podrían contener patrones que no son inmediatamente evidentes, y que  revelen similitudes y diferencias en las formas en que los países se representan a sí mismos a través de sus himnos, así como elementos de la cultura, historia y política de cada país."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Structures\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "#import geopandas as gpd\n",
    "#import json\n",
    "\n",
    "# Corpus Processing\n",
    "import re\n",
    "import nltk.corpus\n",
    "from unidecode                        import unidecode\n",
    "from nltk.tokenize                    import word_tokenize\n",
    "from nltk                             import SnowballStemmer\n",
    "from sklearn.feature_extraction.text  import TfidfVectorizer\n",
    "from sklearn.preprocessing            import normalize\n",
    "\n",
    "# K-Means\n",
    "from sklearn import cluster\n",
    "\n",
    "# Visualization and Analysis\n",
    "import matplotlib.pyplot  as plt\n",
    "import matplotlib.cm      as cm\n",
    "import seaborn            as sns\n",
    "from sklearn.metrics                  import silhouette_samples, silhouette_score\n",
    "from wordcloud                        import WordCloud\n",
    "\n",
    "# Map Viz\n",
    "import folium\n",
    "#import branca.colormap as cm\n",
    "from branca.element import Figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus Loading:\n",
    "\n",
    "Usaremos pandas para leer el archivo CSV que contiene el himno nacional de cada país y su correspondiente código de país. Los himnos fueron extraídos de Wikipedia y muchos de ellos contienen palabras que utilizan caracteres no UTF-8 (generalmente nombres de lugares, etc.), por lo que leeremos el archivo con la codificación _latin1_.\n",
    "\n",
    "Luego extraeremos la columna __Anthem__ en una lista de textos para nuestro corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('datasets/anthems.csv', encoding='utf-8')\n",
    "data.columns = map(str.lower, data.columns)\n",
    "\n",
    "continents = ['Europe', 'South_America', 'North_America']\n",
    "data = data.loc[data['continent'].isin(continents)]\n",
    "data.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = data['anthem'].tolist()\n",
    "corpus[18][0:447]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus Processing\n",
    "\n",
    "### 1. Stop Words and Stemming\n",
    "\n",
    "Realizaremos una rutina de ingeniería de datos con nuestro conjunto de datos de himnos para posteriormente crear un buen modelo. \n",
    "\n",
    "Para ello, eliminaremos todas las palabras que no contribuyan al significado semántico del texto (palabras que no están dentro del alfabeto inglés) y mantendremos todas las palabras restantes en el formato más simple posible, de modo que podamos aplicar una función que asigne pesos a cada palabra sin generar ningún sesgo o valores atípicos. Para ello, existen muchas técnicas para limpiar nuestro corpus, entre ellas, eliminaremos las palabras más comunes (stop words) y aplicaremos stemming, una técnica que reduce una palabra a su raíz.\n",
    "\n",
    "Los métodos que aplican stemming y eliminación de stop words se enumeran a continuación. También definiremos un método que elimine cualquier palabra con menos de 2 letras o más de 21 letras para limpiar aún más nuestro corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removes a list of words (ie. stopwords) from a tokenized list.\n",
    "def removeWords(listOfTokens, listOfWords):\n",
    "    return [token for token in listOfTokens if token not in listOfWords]\n",
    "\n",
    "# applies stemming to a list of tokenized words\n",
    "def applyStemming(listOfTokens, stemmer):\n",
    "    return [stemmer.stem(token) for token in listOfTokens]\n",
    "\n",
    "# removes any words composed of less than 2 or more than 21 letters\n",
    "def twoLetters(listOfTokens):\n",
    "    twoLetterWord = []\n",
    "    for token in listOfTokens:\n",
    "        if len(token) <= 2 or len(token) >= 21:\n",
    "            twoLetterWord.append(token)\n",
    "    return twoLetterWord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. The main corpus processing function.\n",
    "\n",
    "En una sección anterior, al explorar nuestro conjunto de datos, notamos algunas palabras que contenían caracteres extraños que deberían ser eliminados. Utilizando RegEx, nuestra función principal de procesamiento eliminará símbolos ASCII desconocidos, caracteres especiales, números, correos electrónicos, URLs, etc.\n",
    "\n",
    "También utiliza las funciones auxiliares definidas anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processCorpus(corpus, language):   \n",
    "    stopwords = nltk.corpus.stopwords.words(language)\n",
    "    param_stemmer = SnowballStemmer(language)\n",
    "    countries_list = [line.rstrip('\\n') for line in open('lists/countries.txt')] # Load .txt file line by line\n",
    "    nationalities_list = [line.rstrip('\\n') for line in open('lists/nationalities.txt')] # Load .txt file line by line\n",
    "    other_words = [line.rstrip('\\n') for line in open('lists/stopwords_scrapmaker.txt')] # Load .txt file line by line\n",
    "    \n",
    "    for document in corpus:\n",
    "        index = corpus.index(document)\n",
    "        corpus[index] = corpus[index].replace(u'\\ufffd', '8')   # Replaces the ASCII '�' symbol with '8'\n",
    "        corpus[index] = corpus[index].replace(',', '')          # Removes commas\n",
    "        corpus[index] = corpus[index].rstrip('\\n')              # Removes line breaks\n",
    "        corpus[index] = corpus[index].casefold()                # Makes all letters lowercase\n",
    "        \n",
    "        corpus[index] = re.sub('\\W_',' ', corpus[index])        # removes specials characters and leaves only words\n",
    "        corpus[index] = re.sub(\"\\S*\\d\\S*\",\" \", corpus[index])   # removes numbers and words concatenated with numbers IE h4ck3r. Removes road names such as BR-381.\n",
    "        corpus[index] = re.sub(\"\\S*@\\S*\\s?\",\" \", corpus[index]) # removes emails and mentions (words with @)\n",
    "        corpus[index] = re.sub(r'http\\S+', '', corpus[index])   # removes URLs with http\n",
    "        corpus[index] = re.sub(r'www\\S+', '', corpus[index])    # removes URLs with www\n",
    "\n",
    "        listOfTokens = word_tokenize(corpus[index])\n",
    "        twoLetterWord = twoLetters(listOfTokens)\n",
    "\n",
    "        listOfTokens = removeWords(listOfTokens, stopwords)\n",
    "        listOfTokens = removeWords(listOfTokens, twoLetterWord)\n",
    "        listOfTokens = removeWords(listOfTokens, countries_list)\n",
    "        listOfTokens = removeWords(listOfTokens, nationalities_list)\n",
    "        listOfTokens = removeWords(listOfTokens, other_words)\n",
    "        \n",
    "        listOfTokens = applyStemming(listOfTokens, param_stemmer)\n",
    "        listOfTokens = removeWords(listOfTokens, other_words)\n",
    "\n",
    "        corpus[index]   = \" \".join(listOfTokens)\n",
    "        corpus[index] = unidecode(corpus[index])\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = 'english'\n",
    "corpus = processCorpus(corpus, language)\n",
    "corpus[18][0:460]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Weighting of Words\n",
    "\n",
    "Ahora aplicaremos la función TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "tf_idf = pd.DataFrame(data = X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "final_df = tf_idf\n",
    "\n",
    "print(\"{} rows\".format(final_df.shape[0]))\n",
    "final_df.T.nlargest(5, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first 5 words with highest weight on document 0:\n",
    "final_df.T.nlargest(5, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Es momento de descubir los patrones presentes en los datos!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Analysis\n",
    "\n",
    "Now we can take a deeper look at each cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_features_cluster(tf_idf_array, prediction, n_feats):\n",
    "    labels = np.unique(prediction)\n",
    "    dfs = []\n",
    "    for label in labels:\n",
    "        id_temp = np.where(prediction==label) # indices for each cluster\n",
    "        x_means = np.mean(tf_idf_array[id_temp], axis = 0) # returns average score across cluster\n",
    "        sorted_means = np.argsort(x_means)[::-1][:n_feats] # indices with top 20 scores\n",
    "        features = vectorizer.get_feature_names_out()\n",
    "        best_features = [(features[i], x_means[i]) for i in sorted_means]\n",
    "        df = pd.DataFrame(best_features, columns = ['features', 'score'])\n",
    "        dfs.append(df)\n",
    "    return dfs\n",
    "\n",
    "def plotWords(dfs, n_feats):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    for i in range(0, len(dfs)):\n",
    "        plt.title((\"Most Common Words in Cluster {}\".format(i)), fontsize=10, fontweight='bold')\n",
    "        sns.barplot(x = 'score' , y = 'features', orient = 'h' , data = dfs[i][:n_feats])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map of Words\n",
    "\n",
    "Now that we can look at the graphs above and see the best scored words in each cluster, it's also interesting to make it prettier by making a map of words of each cluster!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms a centroids dataframe into a dictionary to be used on a WordCloud.\n",
    "def centroidsDict(centroids, index):\n",
    "    a = centroids.T[index].sort_values(ascending = False).reset_index().values\n",
    "    centroid_dict = dict()\n",
    "\n",
    "    for i in range(0, len(a)):\n",
    "        centroid_dict.update( {a[i,0] : a[i,1]} )\n",
    "\n",
    "    return centroid_dict\n",
    "\n",
    "def generateWordClouds(centroids):\n",
    "    wordcloud = WordCloud(max_font_size=100, background_color = 'white')\n",
    "    for i in range(0, len(centroids)):\n",
    "        centroid_dict = centroidsDict(centroids, i)        \n",
    "        wordcloud.generate_from_frequencies(centroid_dict)\n",
    "\n",
    "        plt.figure()\n",
    "        plt.title('Cluster {}'.format(i))\n",
    "        plt.imshow(wordcloud)\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = pd.DataFrame(kmeans.cluster_centers_)\n",
    "centroids.columns = final_df.columns\n",
    "generateWordClouds(centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing our final groups for visualization\n",
    "\n",
    "Now that we're satisfied with our clustering we should assign which country belongs to which group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning the cluster labels to each country\n",
    "# Create a dataframe named data_to_plot with a 'label' columns indicating the group number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization the Clustered Countries in a Map\n",
    "\n",
    "Now that we have our final grouping it would be really cool to visualize it in a interactive map. To do this we'll use the awesome Folium library to see our interactive map!\n",
    "\n",
    "We'll load a geojson file of polygons and country codes with geopandas and merge it with the labelled dataframe from the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map Viz\n",
    "import json\n",
    "import geopandas as gpd\n",
    "\n",
    "# Loading countries polygons\n",
    "geo_path = 'datasets/world-countries.json'\n",
    "country_geo = json.load(open(geo_path))\n",
    "gpf = gpd.read_file(geo_path)\n",
    "\n",
    "# Merging on the alpha-3 country codes\n",
    "merge = pd.merge(gpf, data, left_on='id', right_on='alpha-3')\n",
    "data_to_plot = merge[[\"id\", \"name\", \"label\", \"geometry\"]]\n",
    "\n",
    "data_to_plot.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create a color_step for each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import branca.colormap as cm\n",
    "\n",
    "# Creating a discrete color map\n",
    "values = data_to_plot[['label']].to_numpy()\n",
    "color_step = cm.StepColormap(['r', 'y','g','b', 'm'], vmin=values.min(), vmax=values.max(), caption='step')\n",
    "\n",
    "color_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Painting the Groups into a Choropleth Map\n",
    "\n",
    "Now that we have all the information that we want to plot into a Dataframe, we'll create a function that makes a Choropleth Map to be displayed on a folium map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from branca.element import Figure\n",
    "\n",
    "def make_geojson_choropleth(display, data, colors):\n",
    "    '''creates geojson choropleth map using a colormap, with tooltip for country names and groups'''\n",
    "    group_dict = data.set_index('id')['label'] # Dictionary of Countries IDs and Clusters\n",
    "    tooltip = folium.features.GeoJsonTooltip([\"name\", \"label\"], aliases=display, labels=True)\n",
    "    return folium.GeoJson(data[[\"id\", \"name\",\"label\",\"geometry\"]],\n",
    "                          style_function = lambda feature: {\n",
    "                               'fillColor': colors(group_dict[feature['properties']['id']]),\n",
    "                               #'fillColor': test(feature),\n",
    "                               'color':'black',\n",
    "                               'weight':0.5\n",
    "                               },\n",
    "                          highlight_function = lambda x: {'weight':2, 'color':'black'},\n",
    "                          smooth_factor=2.0,\n",
    "                          tooltip = tooltip)\n",
    "\n",
    "# Makes map appear inline on notebook\n",
    "def display(m, width, height):\n",
    "    \"\"\"Takes a folium instance and embed HTML.\"\"\"\n",
    "    fig = Figure(width=width, height=height)\n",
    "    fig.add_child(m)\n",
    "    #return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing our Folium Map\n",
    "m = folium.Map(location=[43.5775, -10.106111], zoom_start=2.3, tiles='cartodbpositron')\n",
    "\n",
    "# Making a choropleth map with geojson\n",
    "geojson_choropleth = make_geojson_choropleth([\"Country:\", \"Group:\"], data_to_plot, color_step)\n",
    "geojson_choropleth.add_to(m)\n",
    "\n",
    "width, height = 1300, 675\n",
    "display(m, width, height)\n",
    "m"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
