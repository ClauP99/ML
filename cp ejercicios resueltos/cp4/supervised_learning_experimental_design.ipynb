{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimentación en Aprendizaje Automático:\n",
    "\n",
    "Dada la complejidad de los métodos de aprendizaje automático, estos en general resisten los métodos de análisis formal. Por lo tanto, debemos aprender sobre el comportamiento de los algoritmos en nuestros problemas específicos de manera empírica. \n",
    "\n",
    "**Las respuestas a preguntas que son importantes para ti, como qué algoritmo funciona mejor con tus datos o qué características de entrada utilizar, solo se pueden encontrar a través de los resultados de ensayos experimentales.**\n",
    "\n",
    "Esto representa tanto un desafío cuando nos iniciamos en este campo, como una oportunidad para el descubrimiento y la contribución.\n",
    "\n",
    "La experimentación en aprendizaje automático se trata de realizar ensayos rigurosos y controlados para encontrar el modelo óptimo para un problema específico. Implica variar sistemáticamente las variables independientes, como algoritmos, características o hiperparámetros, y observar su impacto en las métricas de rendimiento.\n",
    "\n",
    "Los experimentos en aprendizaje automático son inherentemente probabilísticos debido a la variabilidad en los datos y en el modelo.\n",
    "\n",
    "**Variabilidad de los datos** surge de las muestras específicas que elegimos para el entrenamiento y la evaluación. \n",
    "\n",
    "**Variabilidad del modelo** proviene de la aleatoriedad inherente a muchos algoritmos, como la inicialización aleatoria de pesos o la aleatorización de los datos.\n",
    "\n",
    "Para obtener buenos resultados en la experimentación, siempre debemos considerar los siguientes principios:\n",
    "\n",
    "1. **Aleatorización**: Para garantizar que cada experimento sea independiente y esté libre de sesgos, debemos aleatorizar la selección de nuestros datos y el orden de los ensayos. Esto asegura que no estemos introduciendo errores sistemáticos de manera involuntaria.\n",
    "2. **Replicación**: Necesitamos repetir nuestros experimentos varias veces, ejecutándolos con diferentes muestras aleatorias de datos y configuraciones de modelo, para comprender el verdadero rango de variabilidad y obtener resultados estadísticamente significativos.\n",
    "3. **Reducción de ruido**: Nuestro objetivo es minimizar la influencia del ruido aleatorio para poder aislar los efectos de nuestras variables independientes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Especificidad vs. generalidad\n",
    "\n",
    "La evaluación de los enfoques de aprendizaje automático (ML) depende en gran medida del alcance previsto de la aplicación. \n",
    "\n",
    "Existen dos enfoques distintos: evaluaciones específicas del dominio y evaluaciones genéricas.\n",
    "\n",
    "**Evaluación Específica del Dominio** se enfoca en adaptar la evaluación a la aplicación objetivo. Esto incluye:\n",
    "\n",
    "- Usar un conjunto de datos que refleje con precisión el escenario del mundo real, asegurando diversidad y capturando variaciones relevantes.\n",
    "- Es apropiado medir el rendimiento utilizando una única métrica en el conjunto de datos elegido. El modelo con el mejor rendimiento en la prueba se considera el mejor para la tarea específica.\n",
    "- Los resultados obtenidos en un conjunto de datos específico del dominio no deben generalizarse a otros contextos.\n",
    "\n",
    "**Evaluación Genérica** tiene como objetivo una comprensión más amplia de las capacidades de un modelo en diversos dominios. Esto implica:\n",
    "\n",
    "- Emplear múltiples conjuntos de datos de diferentes dominios para probar la adaptabilidad del modelo.\n",
    "- Utilizar múltiples métricas de rendimiento para obtener una visión integral de las fortalezas y debilidades del modelo.\n",
    "- Tener cuidado al comparar simplemente los resultados entre conjuntos de datos. Sacar conclusiones sobre el enfoque \"mejor\" en general requiere un análisis más profundo y, posiblemente, un meta-análisis de los resultados.\n",
    "\n",
    "Es crucial reconocer que los resultados de cualquiera de los enfoques no pueden extrapolarse directamente al otro. Los resultados específicos del dominio no deben usarse para reclamar una superioridad general, aunque pueden contribuir a estudios futuros de meta-análisis. De manera similar, los buenos resultados genéricos no garantizan el éxito en aplicaciones específicas. Un enfoque que se ha demostrado ser bueno en promedio en varias tareas puede no tener un buen desempeño en una tarea específica.\n",
    "\n",
    "Realizar una evaluación integral es lo ideal, pero las limitaciones de recursos suelen restringir el alcance de los experimentos. Es importante reconocer estas limitaciones y ajustar las afirmaciones y conclusiones en consecuencia. La clave es evitar hacer afirmaciones que no estén respaldadas por experimentos.\n",
    "\n",
    "Un caso práctico de esto puede consultarse [aquí](https://towardsdatascience.com/a-quick-guide-to-designing-rigorous-machine-learning-experiments-21b19f067703).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Model Selection\n",
    "\n",
    "Al abordar un problema práctico, generalmente podemos pensar en varios algoritmos que podrían ofrecer una buena solución, cada uno de los cuales podría tener varios parámetros. *¿Cómo podemos elegir el mejor algoritmo para el problema en cuestión? ¿Y cómo ajustamos los parámetros del algoritmo?* Esta tarea a menudo se denomina **selección de modelo**.\n",
    "\n",
    "La selección de modelo implica probar diferentes algoritmos y configuraciones de parámetros para determinar cuál ofrece el mejor rendimiento en términos de precisión, generalización y eficiencia para el problema específico. Este proceso incluye la evaluación de los algoritmos en conjuntos de datos de entrenamiento y prueba, la optimización de hiperparámetros y la comparación de métricas de desempeño. El objetivo es encontrar el modelo más adecuado para el problema, considerando tanto su capacidad de aprendizaje como su capacidad de generalizar a nuevos datos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dos problemas relacionados: Model Selection y Hyperparameter Tuning\n",
    "\n",
    "**Model Selection**  \n",
    "Se refiere a la elección de:  \n",
    "- *Características de entrada*: qué variables incluir.\n",
    "- *Preprocesamiento*: qué transformación aplicar (por ejemplo, escalado).  \n",
    "- *Método de aprendizaje automático*: qué algoritmo utilizar.\n",
    "\n",
    "**Hyperparameter Tuning**  \n",
    "Consiste en seleccionar los parámetros específicos del método de aprendizaje automático.  \n",
    "\n",
    "Aunque se distinguen conceptualmente, en la práctica ambos problemas se abordan de manera conjunta. Usamos generalmente validación cruzada para elegir el modelo y los hiperparámetros que minimicen el error en el conjunto de prueba.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluación de modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenar un modelo y evaluarlo utilizando los mismos datos es un error metodológico: un modelo que simplemente memorice las etiquetas de los ejemplos vistos durante el entrenamiento obtendría una puntuación perfecta, pero sería incapaz de generalizar a datos nuevos. \n",
    "A este problema se le conoce como **sobreajuste**.  \n",
    "\n",
    "Para evitarlo, es habitual en experimentos de aprendizaje supervisado separar una parte de los datos disponibles como conjunto de prueba (`X_test`, `y_test`), que se utiliza exclusivamente para evaluar el desempeño del modelo. \n",
    "\n",
    "En *scikit-learn*, se puede realizar rápidamente una división aleatoria entre el conjunto de entrenamiento y el de prueba utilizando la función auxiliar `train_test_split`.  \\\n",
    "Esta función permite especificar la proporción de datos que se incluirán en el conjunto de prueba, mezclar los datos y establecer una semilla aleatoria para garantizar la reproducibilidad.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Al evaluar diferentes configuraciones (*“hiperparámetros”*) para los estimadores, sigue existiendo el riesgo de sobreajuste en el conjunto de prueba, ya que los parámetros pueden ajustarse hasta que el estimador rinda de manera óptima. \\\n",
    "De esta forma, el conocimiento sobre el conjunto de prueba puede \"filtrarse\" hacia el modelo y las métricas de evaluación ya no reflejarán el desempeño de generalización.  \n",
    "\n",
    "Para resolver este problema, se puede reservar otra parte del conjunto de datos como un **“conjunto de validación”**: el entrenamiento se realiza sobre el conjunto de entrenamiento, luego se evalúa en el conjunto de validación, y cuando el experimento parece exitoso, se puede realizar la evaluación final sobre el conjunto de prueba.\n",
    "\n",
    "Ambas técnicas las habíamos utilizado en clases anteriores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*¿Qué desventajas tiene este último enfoque?*\n",
    "\n",
    "- Al dividir los datos disponibles en tres conjuntos, reducimos drásticamente el número de muestras que se pueden utilizar para entrenar el modelo.  \n",
    "- Los resultados pueden depender de una elección aleatoria particular para el par de conjuntos (entrenamiento, validación).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una solución a este problema es un procedimiento llamado **validación cruzada** (*cross-validation* o CV, por su nombre en inglés). \n",
    "Aún debe reservarse un conjunto de prueba para la evaluación final, pero el conjunto de validación ya no es necesario al realizar CV.  \n",
    "\n",
    "En el enfoque básico, llamado **k-fold CV**, el conjunto de entrenamiento se divide en k conjuntos más pequeños. El siguiente procedimiento se sigue para cada uno de los k grupos:\n",
    "\n",
    "1. Se entrena un modelo utilizando *k-1* de los conjuntos como datos de entrenamiento.  \n",
    "2. El modelo resultante se valida sobre el conjunto restante de los datos (es decir, se utiliza como un conjunto de prueba para calcular las medidas de desempeño).\n",
    "\n",
    "La medida de desempeño reportada por la validación cruzada *k-fold* es el promedio de los valores calculados en el ciclo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, se muestra una figura que ilustra el procedimiento para **k = 5**:\n",
    "\n",
    "\n",
    "<img src=\"images/kfold.png\" alt=\"image\" width=\"auto\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**Ventajas de la validación cruzada k-fold:**\n",
    "\n",
    "- *Uso eficiente de los datos*: Todos los datos se utilizan tanto para entrenar como para validar el modelo, lo que es crucial cuando los datos son limitados.\n",
    "\n",
    "- *Evaluación más robusta*: Al realizar múltiples entrenamientos y evaluaciones con diferentes particiones, se obtiene una estimación más precisa del desempeño del modelo, menos influenciada por la partición aleatoria.\n",
    "\n",
    "- *Reducción del sesgo*: Al evaluar el modelo en diferentes subconjuntos de datos, se reduce el riesgo de que el modelo se sobreajuste a una partición específica del conjunto de validación.\n",
    "\n",
    "**Desventajas de la validación cruzada k-fold:**\n",
    "\n",
    "- *Costo computacional* El entrenamiento y la validación deben realizarse k veces, lo que puede ser costoso en términos de tiempo y recursos computacionales, especialmente con modelos complejos o grandes volúmenes de datos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "*Esta estrategia básica de validación cruzada es factible para todos los tipos de datos y problemas?*\n",
    "\n",
    "*En qué casos crees que podría no funcionar?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**Estrategias de Validación Cruzada**\n",
    "\n",
    "Scikit-learn ofrece varias estrategias de validación cruzada, cada una diseñada para adaptarse a diferentes tipos de conjuntos de datos y requisitos de validación de modelos. \n",
    "\n",
    "A continuación se presentan algunas de las estrategias clave:\n",
    "\n",
    "1. **Validación Cruzada K-Folds**:\n",
    "   - El conjunto de datos se divide en K subconjuntos (folds).\n",
    "   - El modelo se entrena con K-1 subconjuntos y se prueba con el subconjunto restante.\n",
    "   - Este proceso se repite K veces, con cada fold sirviendo una vez como conjunto de prueba.\n",
    "   - Proporciona una estimación confiable del rendimiento del modelo.\n",
    "\n",
    "2. **Validación Cruzada K-Folds Estratificada**:\n",
    "   - Similar a K-folds, pero asegura que cada fold mantenga la misma distribución de la variable objetivo que el conjunto de datos original.\n",
    "   - Es especialmente útil para tareas de clasificación desequilibradas.\n",
    "\n",
    "3. **Validación Cruzada Leave-One-Out (LOO-CV)**:\n",
    "   - Cada elemento del conjunto de datos se usa como conjunto de prueba, mientras que los restantes se usan para entrenar.\n",
    "   - Es útil cuando se trabaja con conjuntos de datos pequeños, pero es costoso computacionalmente.\n",
    "\n",
    "4. **Validación Cruzada Leave-P-Out**:\n",
    "   - Similar a LOO-CV, pero prueba con P elementos de datos dejados afuera, en lugar de solo uno.\n",
    "   - Proporciona una estimación más amplia de la capacidad de generalización del modelo.\n",
    "\n",
    "5. **Validación Cruzada ShuffleSplit**:\n",
    "   - Este método divide aleatoriamente los datos en conjuntos de entrenamiento y prueba varias veces.\n",
    "   - A diferencia de K-folds, los conjuntos de entrenamiento y prueba pueden superponerse entre divisiones, ofreciendo mayor variabilidad.\n",
    "\n",
    "6. **Group K-Folds**:\n",
    "   - La suposición de independencia e idéntica distribución (i.i.d.) se rompe cuando el proceso generador subyacente produce grupos de muestras dependientes. \n",
    "   - En este caso, los datos están agrupados en conjuntos distintos. Por ejemplo, si una misma prueba médica proviene de hospitales o equipos diferentes, cada fuente podría ser un grupo. Si se dividieran aleatoriamente los datos en los diferentes *folds*, podría ocurrir que un mismo grupo (por ejemplo, todas las muestras de un hospital X) apareciera tanto en el conjunto de entrenamiento como en el de prueba, lo que podría llevar a una evaluación sesgada del modelo.\n",
    "   - En este caso, nos gustaría saber si un modelo entrenado con un conjunto particular de grupos generaliza bien a los grupos no vistos. Para medir esto, necesitamos asegurarnos de que todas las muestras en el fold de validación provengan de grupos que no estén representados en absoluto en el fold de entrenamiento correspondiente.\n",
    "\n",
    "7. **TimeSeriesSplit**:\n",
    "   - Diseñada específicamente para datos de series temporales donde no se deben usar valores futuros para predecir valores pasados.\n",
    "   - Este método divide los datos en un número fijo de divisiones, asegurando que el modelo siempre se entrene con puntos de datos previos al conjunto de prueba, respetando el orden temporal.\n",
    "   - TimeSeriesSplit es una variación de k-fold que devuelve los primeros k folds como conjunto de entrenamiento y el fold k+1 como conjunto de prueba. \n",
    "   - A diferencia de los métodos de validación cruzada estándar, los conjuntos de entrenamiento sucesivos son supersets de aquellos que vienen antes que ellos.\n",
    "\n",
    "Estas estrategias ayudan a mitigar el sobreajuste, garantizan una evaluación justa del modelo y se eligen según la estructura y características del conjunto de datos.\n",
    "\n",
    "Para más detalles sobre estas estrategias y ejemplos, consulta la [documentación oficial de scikit-learn](https://scikit-learn.org/1.5/modules/cross_validation.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, se muestra una figura que ilustra el procedimiento general para seleccionar un modelo:\n",
    "\n",
    "<img src=\"images/grid_search_workflow.png\" alt=\"image\" width=\"auto\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimadores Dummy\n",
    "\n",
    "Cuando calculas tus primeros resultados de validación cruzada para estimar el rendimiento de tu modelo, generalmente sabes que, mientras mayor sea la puntuación, mejor. Si la puntuación es bastante alta en el primer intento, eso es excelente, pero no suele ser el caso.\n",
    "\n",
    "*¿Qué hacer si la primera puntuación de precisión es bastante baja o inferior a lo que deseas o esperas? ¿Es un problema de los datos? ¿Es el modelo? ¿Ambos? ¿Cómo podemos saber rápidamente si nuestro modelo está mal ajustado?*\n",
    "\n",
    "Aquí es donde entran los **modelos dummy**. Su complejidad e \"inteligencia\" son muy bajas: la idea es que puedes comparar tu modelo con ellos para ver cuánto mejor es en comparación con los modelos más *simples o ingenuos*. Cabe destacar que los modelos dummy no predicen intencionalmente valores absurdos, sino que hacen suposiciones muy simples y directas.\n",
    "\n",
    "Si tu modelo tiene un rendimiento peor que el modelo dummy, deberías ajustar o cambiar completamente tu modelo.\n",
    "\n",
    "`DummyClassifier` de **scikit-learn** implementa varias de estas estrategias simples para clasificación:\n",
    "\n",
    "- `stratified` genera predicciones aleatorias respetando la distribución de clases del conjunto de entrenamiento.\n",
    "- `most_frequent` siempre predice la etiqueta más frecuente en el conjunto de entrenamiento.\n",
    "- `prior` siempre predice la clase que maximiza la probabilidad a priori de la clase (como `most_frequent`), y `predict_proba` devuelve la probabilidad a priori de la clase.\n",
    "- `uniform` genera predicciones aleatorias de manera uniforme.\n",
    "- `constant` siempre predice una etiqueta constante proporcionada por el usuario. Una motivación principal de este método es el cálculo del F1-score, cuando la clase positiva es minoritaria.\n",
    "\n",
    "`DummyClassifier` se utiliza principalmente para crear modelos de referencia y comparar el rendimiento de modelos más complejos con un modelo básico que no realiza un aprendizaje real. Esto ayuda a verificar si el modelo más avanzado está aprendiendo de manera significativa y no simplemente generando predicciones triviales o aleatorias.\n",
    "\n",
    "El `DummyRegressor` también implementa cuatro reglas simples de referencia para regresión:\n",
    "\n",
    "- `mean`: siempre predice la media de los valores objetivo del conjunto de entrenamiento.\n",
    "- `median`: siempre predice la mediana de los valores objetivo del conjunto de entrenamiento.\n",
    "- `quantile`: siempre predice un cuantil especificado por el usuario a partir de los valores objetivo del conjunto de entrenamiento.\n",
    "- `constant`: siempre predice un valor constante proporcionado por el usuario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selección de hiperparámetros\n",
    "\n",
    "Los **Hyper-parameters** son parámetros que no se aprenden directamente dentro de los estimadores. Ejemplos típicos incluyen `C`, `kernel` y `gamma` para clasificadores de vectores de soporte (SVC), o `k` para K-Nearest Neighbors (KNN), entre otros.\n",
    "\n",
    "El **ajuste de hiperparámetros** es un proceso crítico en el desarrollo de modelos de machine learning. Consiste en encontrar la configuración óptima de hiperparámetros para el comportamiento de un algoritmo de aprendizaje automático. Estos parámetros tienen un impacto significativo en el rendimiento del modelo, y elegir los hiperparámetros correctos puede ser la diferencia entre un modelo poco efectivo y uno que alcance resultados de vanguardia.\n",
    "\n",
    "Es posible y recomendable buscar en el espacio de hiperparámetros para encontrar la mejor puntuación de validación cruzada. Cualquier parámetro proporcionado al construir un estimador puede optimizarse de esta manera. \n",
    "\n",
    "Específicamente, para obtener los nombres y valores actuales de todos los parámetros de un estimador dado, se puede usar:\n",
    "\n",
    "```python\n",
    "estimator.get_params()\n",
    "```\n",
    "\n",
    "Un proceso de búsqueda consta de:\n",
    "\n",
    "- Un estimador (regresor o clasificador, como `sklearn.svm.SVC()`).\n",
    "- Un espacio de parámetros.\n",
    "- Un método para buscar o muestrear candidatos.\n",
    "- Un esquema de validación cruzada.\n",
    "- Una función de puntuación.\n",
    "\n",
    "En **scikit-learn** se proporcionan dos enfoques genéricos para la búsqueda de parámetros:\n",
    "\n",
    "1. **`GridSearchCV`**: considera exhaustivamente todas las combinaciones de parámetros en un espacio definido.\n",
    "2. **`RandomizedSearchCV`**: muestrea un número dado de candidatos del espacio de parámetros con una distribución especificada.\n",
    "\n",
    "Ambas herramientas tienen contrapartes de **halving**: `HalvingGridSearchCV` y `HalvingRandomSearchCV`, que pueden ser mucho más rápidas para encontrar una buena combinación de parámetros.\n",
    "\n",
    "Para más detalles sobre estas estrategias y tips, consulta la [documentación oficial](https://scikit-learn.org/1.5/modules/grid_search.html).\n",
    "\n",
    "Existen varias estrategias y herramientas más allá de `GridSearchCV` y `RandomizedSearchCV` para optimizar hiperparámetros. Algunos ejemplos son:\n",
    "\n",
    "**Búsqueda Bayesiana**  \n",
    "   Este método utiliza modelos probabilísticos para construir una función de probabilidad del espacio de hiperparámetros y selecciona las configuraciones que probablemente optimicen el modelo.\n",
    "\n",
    "**Optimización Evolutiva (Algoritmos Genéticos)**  \n",
    "   Se basa en la evolución natural para iterar sobre configuraciones de hiperparámetros, utilizando operaciones como selección, cruce y mutación.\n",
    "\n",
    "**AutoML**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curva de Validación\n",
    "\n",
    "Para validar un modelo, se necesita una función de puntuación, como *accuracy* para clasificadores.  \n",
    "\n",
    "La forma adecuada para seleccionar múltiples hiperparámetros de un estimador es a través de **grid search** u otros métodos similares. Estos métodos eligen los hiperparámetros que maximizan el puntaje en un conjunto de validación o múltiples conjuntos de validación.  \n",
    "\n",
    "Sin embargo, a veces, resulta útil graficar cómo un único hiperparámetro influye en los puntajes de entrenamiento y validación para identificar si el modelo está sobreajustando o subajustando a ciertos valores del hiperparámetro.\n",
    "\n",
    "*¿Qué es una curva de validación?*\n",
    "\n",
    "Una **curva de validación** muestra cómo el cambio en un hiperparámetro afecta el rendimiento del modelo.  \n",
    "\n",
    "Nos permite analizar la relación entre la complejidad del modelo (como el número de características o la profundidad de un árbol de decisión) y el rendimiento en los datos de entrenamiento y validación.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "*¿Cómo crear una curva de validación?*\n",
    "\n",
    "1. Elegir un hiperparámetro.\n",
    "2. Entrenar varios modelos con diferentes valores de ese parámetro.\n",
    "3. Medir el rendimiento de cada modelo en los datos de entrenamiento y validación.\n",
    "4. Graficar los resultados obtenidos.\n",
    "\n",
    "*Curva de Validación en Scikit-learn*\n",
    "\n",
    "Scikit-learn proporciona la función `validation_curve` para generar curvas de validación de manera sencilla. \n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "train_scores, valid_scores = validation_curve(\n",
    "    estimator,          # El modelo a evaluar, por ejemplo, SVC()\n",
    "    X,                  # Características (features) del conjunto de datos de entrenamiento\n",
    "    y,                  # Etiquetas del conjunto de datos de entrenamiento\n",
    "    param_name,         # Nombre del hiperparámetro a variar (str)\n",
    "    param_range,        # Lista de valores para el hiperparámetro\n",
    "    scoring=None,       # Métrica de evaluación (opcional, por defecto usa la predeterminada de    estimador)\n",
    "    cv=5                # Número de particiones de validación cruzada\n",
    ")\n",
    "\n",
    "```\n",
    "En lugar de graficar manualmente los resultados de `validation_curve` con `matplotlib`, Scikit-learn ofrece la clase `ValidationCurveDisplay`. Esta es una opción más directa y sencilla para visualizar las curvas de validación sin necesidad de manejar directamente los resultados con matplotlib.\n",
    "\n",
    "A continuación se muestra una imagen de ejemplo de una curva de validación para ilustrar cómo el cambio en los hiperparámetros afecta el rendimiento del modelo en los datos de entrenamiento y validación.\n",
    "\n",
    "![Curva de Validación de Ejemplo](images/val_curve-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Interpretación de la curva de validación*:\n",
    "- **Detección de sobreajuste**:  \n",
    "  Si el rendimiento en entrenamiento es significativamente mejor que en validación, el modelo está sobreajustando. Esto indica que el modelo está memorizando los datos de entrenamiento, pero no generaliza bien a datos nuevos.\n",
    "\n",
    "- **Detección de subajuste**:  \n",
    "  Si el rendimiento en entrenamiento y validación son bajos, el modelo está subajustando. Esto sugiere que el modelo es demasiado simple para capturar los patrones en los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curva de Aprendizaje\n",
    "\n",
    "Una **curva de aprendizaje** muestra cómo varían la puntuación de entrenamiento y validación de un estimador a medida que cambia el número de muestras de entrenamiento utilizadas. Es una herramienta muy útil para determinar:\n",
    "\n",
    "- **Qué tanto se beneficia el modelo al agregar más datos de entrenamiento.**  \n",
    "- **Si el modelo tiene más problemas con errores por varianza o por sesgo (bias).**\n",
    "\n",
    "*Generación de la Curva de Aprendizaje*\n",
    "\n",
    "Para obtener los valores necesarios para graficar la curva de aprendizaje (número de muestras utilizadas, la media de las puntuaciones en los conjuntos de entrenamiento y validación), se puede utilizar la función `learning_curve`.\n",
    "\n",
    "Si solo necesitas graficar la curva de aprendizaje, es más sencillo utilizar la clase `LearningCurveDisplay`.\n",
    "\n",
    "La siguiente imagen ilustra lo que se considera una *curva de aprendizaje ideal* en el contexto de entrenamiento de modelos de Machine Learning:\n",
    "\n",
    "<img src=\"images/Learning curve.png\" alt=\"image\" width=\"auto\" height=\"300\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretabilidad de los modelos de aprendizaje\n",
    "\n",
    "En **Machine Learning (ML)**, la **interpretabilidad** y la **explicabilidad** son conceptos fundamentales para comprender cómo un modelo toma sus decisiones. Esto es crucial porque permite:\n",
    "\n",
    "- Generar confianza en los resultados.\n",
    "- Detectar y corregir sesgos.\n",
    "- Diagnosticar debilidades para mejorar el rendimiento.\n",
    "- Cumplir con regulaciones legales y éticas.\n",
    "\n",
    "En aplicaciones críticas como **medicina, finanzas o justicia**, entender por qué un modelo toma una decisión es esencial para evitar errores y asegurar la transparencia. Además, estas prácticas permiten:\n",
    "\n",
    "- Optimizar los modelos.\n",
    "- Identificar sus limitaciones.\n",
    "- Garantizar que sus decisiones sean responsables y seguras para los usuarios.\n",
    "\n",
    "En el notebook [`Metodos_de_Analisis.ipynb`](Metodos_de_Analisis.ipynb) se exploran y relacionan algunas de estas técnicas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
